{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import collections\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from used_metric import get_performance\n",
    "\n",
    "r_path = './douban/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, path=r_path, batch_size = 2048, step = 100):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.step = step\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.train_user_list = collections.defaultdict(list)\n",
    "        self.train_user_list_time = collections.defaultdict(list)\n",
    "        self.train_item_list = collections.defaultdict(list)\n",
    "        \n",
    "        self.test_user_list = collections.defaultdict(list)\n",
    "        self.test_item_list = collections.defaultdict(list)\n",
    "        \n",
    "        self.load_train_data()\n",
    "        self.load_test_data()\n",
    "        \n",
    "        self.n_users = self.n_users + 1\n",
    "        self.n_items = self.n_items + 1\n",
    "        self.users = list(range(self.n_users))\n",
    "        self.items = list(range(self.n_items))\n",
    "        \n",
    "        self.length = self.batch_size * self.step\n",
    "        \n",
    "    def add_expo_popularity(self,popularity):\n",
    "        self.expo_popularity = popularity\n",
    "        \n",
    "    def load_train_data(self):          \n",
    "        train_file = self.path + 'train_with_time.txt'        \n",
    "        train_data = pd.read_csv(train_file, header=None, sep=' ')\n",
    "        train_data.columns = ['uid','iid','time','stars']\n",
    "        train_data = train_data[['uid','iid','time']]\n",
    "        unique_time = train_data['time'].unique()\n",
    "        print(\"time slot unique in train:\",unique_time)\n",
    "        self.unique_times = list(unique_time)\n",
    "        if train_data['time'].unique().shape[0] < 2:\n",
    "            raise RuntimeWarning(\"there only one time slot for train...., this may cause our method not work\")\n",
    "\n",
    "        for col in train_data.columns:\n",
    "            train_data[col] = train_data[col].astype(int)\n",
    "        user_item_time = train_data.groupby('uid')[['iid','time']].agg(list)\n",
    "        self.train_user_list = dict(zip(user_item_time.index,user_item_time['iid']))\n",
    "        self.train_user_list_time = dict(zip(user_item_time.index,user_item_time['time']))\n",
    "        item_user = train_data.groupby('iid')[['uid','time']].agg(list)\n",
    "        print(item_user.head(2))\n",
    "        self.train_item_list = dict(zip(item_user.index,item_user['uid']))\n",
    "\n",
    "        self.n_users = max(self.n_users,train_data['uid'].max())\n",
    "        self.n_items = max(self.n_items,train_data['iid'].max())\n",
    "        \n",
    "    def load_test_data(self):\n",
    "        test_file = self.path + 'test.txt'\n",
    "        with open(test_file) as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n').split(' ')\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                line = [int(i) for i in line]\n",
    "                user = line[0]\n",
    "                items = line[1:]\n",
    "                if (len(items)==0):\n",
    "                    continue\n",
    "                self.test_user_list[user] = items\n",
    "                for item in items:\n",
    "                    self.test_item_list[item].append(user)\n",
    "                self.n_users = max(self.n_users, user)\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = random.choice(self.users) \n",
    "\n",
    "        neg_item = None\n",
    "        \n",
    "        pos_items = self.train_user_list[user]\n",
    "        clicked_times = self.train_user_list_time[user]\n",
    "        \n",
    "        random_index = rd.randint(0,len(pos_items) - 1)\n",
    "        pos_item = pos_items[random_index]\n",
    "        pos_time = clicked_times[random_index]\n",
    "    \n",
    "        while neg_item is None or neg_item in pos_items:\n",
    "            neg_item = rd.choice(self.items)\n",
    "      \n",
    "        pos_pop = self.expo_popularity[pos_item,pos_time]\n",
    "        neg_pop = self.expo_popularity[neg_item,pos_time]   \n",
    "    \n",
    "        return user, pos_item, neg_item, pos_pop, neg_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluation():\n",
    "    def __init__(self, data, Ks, batch_size):\n",
    "        self.data = data\n",
    "        self.Ks = Ks\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def test_one_batch(self, model, batch_user):\n",
    "        batch_user = batch_user.to(device)\n",
    "        batch_rec = model.do_recommendation(batch_user)\n",
    "        batch_rec = torch.sigmoid(batch_rec)\n",
    "        mask = torch.ones_like(batch_rec)\n",
    "        for i in range(len(batch_user)):\n",
    "            mask[i].scatter_(dim = 0, index=torch.tensor(list(self.data.train_user_list[batch_user[i].item()])).to(device), value=torch.tensor(0.0).to(device))\n",
    "        batch_rec = torch.mul(mask, batch_rec)\n",
    "        _, batch_rec = torch.sort(batch_rec, descending=True)\n",
    "        batch_rec = batch_rec.cpu().numpy()\n",
    "        result = {'precision': np.zeros(len(self.Ks)), 'recall': np.zeros(len(self.Ks)), 'ndcg': np.zeros(len(self.Ks)),\n",
    "                  'hit_ratio': np.zeros(len(self.Ks))}\n",
    "        for i in range(len(batch_user)):\n",
    "            u = batch_user[i].item()\n",
    "            r = batch_rec[i]\n",
    "            u_target = self.data.test_user_list[u]\n",
    "            one_user_result = get_performance(u_target, r, self.Ks)\n",
    "            result['precision'] += one_user_result['precision']\n",
    "            result['recall'] += one_user_result['recall']\n",
    "            result['ndcg'] += one_user_result['ndcg']\n",
    "            result['hit_ratio'] += one_user_result['hit_ratio']\n",
    "        return result\n",
    "\n",
    "    def eval(self, model):\n",
    "        result = {'precision': np.zeros(len(self.Ks)), 'recall': np.zeros(len(self.Ks)), 'ndcg': np.zeros(len(self.Ks)),\n",
    "                  'hit_ratio': np.zeros(len(self.Ks))}\n",
    "        all_users = list(self.data.test_user_list.keys())\n",
    "        tot_users = len(all_users)\n",
    "        for i in range(0, tot_users, self.batch_size):\n",
    "            end_idx = min(i + self.batch_size, tot_users)\n",
    "            batch_user = torch.tensor(all_users[i:end_idx], dtype=torch.long)\n",
    "            res = self.test_one_batch(model, batch_user)\n",
    "            \n",
    "            result['precision'] += res['precision']/tot_users\n",
    "            result['recall'] += res['recall']/tot_users\n",
    "            result['ndcg'] += res['ndcg']/tot_users\n",
    "            result['hit_ratio'] += res['hit_ratio']/tot_users\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_popularity():\n",
    "    pop_save_path = r_path + \"item_pop_seq_ori2.txt\"\n",
    "    print(\"popularity used:\",pop_save_path)\n",
    "    with open(pop_save_path) as f:\n",
    "        print(\"pop save path: \", pop_save_path)\n",
    "        item_list = []\n",
    "        pop_item_all = []\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            item, pop_list = int(line[0]), [float(x) for x in line[1:]]\n",
    "            item_list.append(item)\n",
    "            pop_item_all.append(pop_list)\n",
    "    pop_item_all = np.array(pop_item_all)\n",
    "    print(\"pop_item_all shape:\", pop_item_all.shape)\n",
    "    print(\"load pop information:\",pop_item_all.mean(),pop_item_all.max(),pop_item_all.min())\n",
    "    return pop_item_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity used: ./autodl-tmp/douban_movie/popularity_distribution.txt\n",
      "pop save path:  ./autodl-tmp/douban_movie/popularity_distribution.txt\n",
      "pop_item_all shape: (26047, 10)\n",
      "load pop information: 0.0037181866982990796 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "pop_item_all = load_popularity()\n",
    "last_stage_popualarity = pop_item_all[:,-2]\n",
    "popularity_exp = 0.02\n",
    "popularity_matrix = pop_item_all[:,:-1]\n",
    "popularity_matrix = np.power(popularity_matrix,popularity_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBPRMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size, weight_decay):\n",
    "        super(ConditionalBPRMF, self).__init__()\n",
    "        self.n_users = data.n_users\n",
    "        self.n_items = data.n_items\n",
    "\n",
    "        self.decay = weight_decay\n",
    "        self.emb_dim = emb_size\n",
    "\n",
    "        self.user_embedding = nn.Embedding(self.n_users, self.emb_dim)\n",
    "        self.item_embedding = nn.Embedding(self.n_items, self.emb_dim) \n",
    "        \n",
    "        self.user_embedding.weight.data.uniform_(0, 0.005)  # 0-0.005之间均匀分布\n",
    "        self.item_embedding.weight.data.uniform_(0, 0.005)\n",
    "    \n",
    "    def forward(self, users, pos_items, neg_items, pos_pop, neg_pop):\n",
    "        user_embedding = self.user_embedding(users)\n",
    "        pos_item_embedding = self.item_embedding(pos_items)\n",
    "        neg_item_embedding = self.item_embedding(neg_items)\n",
    "\n",
    "        pos_scores = torch.sum(user_embedding * pos_item_embedding, dim=1)\n",
    "        neg_scores = torch.sum(user_embedding * neg_item_embedding, dim=1)\n",
    "\n",
    "        pos_scores = torch.nn.functional.elu(pos_scores) + 1\n",
    "        neg_scores = torch.nn.functional.elu(neg_scores) + 1\n",
    "        pos_scores_with_pop = pos_scores * pos_pop\n",
    "        neg_scores_with_pop = neg_scores * neg_pop\n",
    "\n",
    "        bpr_loss = -torch.mean(torch.log(torch.sigmoid(pos_scores_with_pop - neg_scores_with_pop)))\n",
    "        \n",
    "        regularizer = torch.norm(user_embedding, p=2) + torch.norm(pos_item_embedding, p=2) + torch.norm(neg_item_embedding, p=2)\n",
    "        reg_loss = self.decay * regularizer\n",
    "        return bpr_loss + reg_loss\n",
    "    \n",
    "    def do_recommendation(self, users):\n",
    "        user_emb = self.user_embedding(users)\n",
    "        item_embs = self.item_embedding.weight\n",
    "        scores = torch.mm(user_emb, item_embs.t())\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time slot unique in train: [0 1 2 5 3 4 6 7 8]\n",
      "                                                   uid  \\\n",
      "iid                                                      \n",
      "0    [0, 3, 6, 10, 11, 16, 20, 22, 25, 28, 30, 32, ...   \n",
      "1    [0, 1, 40, 60, 80, 83, 91, 106, 108, 114, 124,...   \n",
      "\n",
      "                                                  time  \n",
      "iid                                                     \n",
      "0    [0, 0, 8, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
      "1    [0, 7, 0, 0, 7, 4, 7, 6, 1, 7, 1, 3, 2, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "data = Data(batch_size = 2048, step = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_expo_popularity(popularity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size=2048, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation(data,[20], 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalBPRMF(data.n_users, data.n_items, emb_size = 32, weight_decay = 1e-04).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss: 53.28260979806472\n",
      "{'precision': array([0.04032177]), 'recall': array([0.03914023]), 'ndcg': array([0.05334784]), 'hit_ratio': array([0.36083636])}\n",
      "Running time: 547.5436552911997 Seconds\n",
      "\n",
      "epoch: 120, loss: 50.652926401691566\n",
      "{'precision': array([0.03970828]), 'recall': array([0.03792956]), 'ndcg': array([0.05035846]), 'hit_ratio': array([0.35582822])}\n",
      "Running time: 124.95018323510885 Seconds\n",
      "\n",
      "epoch: 140, loss: 48.54335732552048\n",
      "{'precision': array([0.04145799]), 'recall': array([0.04020134]), 'ndcg': array([0.05384049]), 'hit_ratio': array([0.3659071])}\n",
      "Running time: 125.34669161587954 Seconds\n",
      "\n",
      "epoch: 160, loss: 47.066732179190716\n",
      "{'precision': array([0.04216539]), 'recall': array([0.04059176]), 'ndcg': array([0.055362]), 'hit_ratio': array([0.36797296])}\n",
      "Running time: 119.3810528293252 Seconds\n",
      "\n",
      "epoch: 180, loss: 45.8281084993628\n",
      "{'precision': array([0.04208714]), 'recall': array([0.0413063]), 'ndcg': array([0.05463329]), 'hit_ratio': array([0.37104044])}\n",
      "Running time: 119.507517747581 Seconds\n",
      "\n",
      "epoch: 200, loss: 44.87186706664192\n",
      "{'precision': array([0.04363653]), 'recall': array([0.04281663]), 'ndcg': array([0.05760591]), 'hit_ratio': array([0.37836484])}\n",
      "Running time: 118.31942281126976 Seconds\n",
      "\n",
      "epoch: 220, loss: 44.136546733072116\n",
      "{'precision': array([0.04323588]), 'recall': array([0.04214861]), 'ndcg': array([0.05640529]), 'hit_ratio': array([0.37736322])}\n",
      "Running time: 117.37689709663391 Seconds\n",
      "\n",
      "epoch: 240, loss: 43.4662739714394\n",
      "{'precision': array([0.0431232]), 'recall': array([0.04145712]), 'ndcg': array([0.0564819]), 'hit_ratio': array([0.37773883])}\n",
      "Running time: 118.17228512465954 Seconds\n",
      "\n",
      "epoch: 260, loss: 43.06643412122877\n",
      "{'precision': array([0.04440967]), 'recall': array([0.04308973]), 'ndcg': array([0.05719233]), 'hit_ratio': array([0.38011769])}\n",
      "Running time: 118.77127190679312 Seconds\n",
      "\n",
      "epoch: 280, loss: 42.48551963228755\n",
      "{'precision': array([0.04418743]), 'recall': array([0.04313265]), 'ndcg': array([0.05708937]), 'hit_ratio': array([0.38080631])}\n",
      "Running time: 119.96634486317635 Seconds\n",
      "\n",
      "epoch: 300, loss: 42.02282623713665\n",
      "{'precision': array([0.0440591]), 'recall': array([0.04293693]), 'ndcg': array([0.05671037]), 'hit_ratio': array([0.38162013])}\n",
      "Running time: 119.13634710013866 Seconds\n",
      "\n",
      "epoch: 320, loss: 41.74307965484975\n",
      "{'precision': array([0.04462877]), 'recall': array([0.04364471]), 'ndcg': array([0.05950326]), 'hit_ratio': array([0.38155753])}\n",
      "Running time: 120.21723603457212 Seconds\n",
      "\n",
      "epoch: 340, loss: 41.64140479331194\n",
      "{'precision': array([0.04590585]), 'recall': array([0.04555257]), 'ndcg': array([0.0608621]), 'hit_ratio': array([0.39232503])}\n",
      "Running time: 119.98402427881956 Seconds\n",
      "\n",
      "epoch: 360, loss: 41.34586655276392\n",
      "{'precision': array([0.04666333]), 'recall': array([0.04579098]), 'ndcg': array([0.06183044]), 'hit_ratio': array([0.39263804])}\n",
      "Running time: 117.46637756377459 Seconds\n",
      "\n",
      "epoch: 380, loss: 41.21779854788879\n",
      "{'precision': array([0.04512333]), 'recall': array([0.04351345]), 'ndcg': array([0.0587153]), 'hit_ratio': array([0.38525103])}\n",
      "Running time: 120.05132975429296 Seconds\n",
      "\n",
      "epoch: 400, loss: 40.90150341628378\n",
      "{'precision': array([0.04628459]), 'recall': array([0.04551282]), 'ndcg': array([0.060637]), 'hit_ratio': array([0.39288844])}\n",
      "Running time: 118.92129091918468 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "EPOCH = 400\n",
    "for epoch in range(EPOCH):\n",
    "    t_loss = 0.0\n",
    "    for idx, (users, pos_items, neg_items, pos_pop, neg_pop) in enumerate(dataloader):\n",
    "        users = users.to(device)\n",
    "        pos_items = pos_items.to(device)\n",
    "        neg_items = neg_items.to(device)\n",
    "        pos_pop = pos_pop.to(device)\n",
    "        neg_pop = neg_pop.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(users, pos_items, neg_items, pos_pop, neg_pop)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "        \n",
    "    if (1+epoch) % 20 == 0 and (1+epoch) >= 100 :\n",
    "        res = evaluator.eval(model)\n",
    "        print(f'epoch: {epoch+1}, loss: {t_loss}')\n",
    "        print(res)\n",
    "        end = time.perf_counter()\n",
    "        print('Running time: %s Seconds\\n' % (end - start))\n",
    "        start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'PD.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
